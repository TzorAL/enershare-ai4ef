{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from enum import IntEnum\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import tempfile\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from darts.models import NaiveSeasonal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.profiler import SimpleProfiler\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "import logging\n",
    "import click\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "from mlflow.models.signature import infer_signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(dframe):\n",
    "\n",
    "    # Remove date column (not needed)\n",
    "    dframe.drop('The data',axis=1,inplace=True)\n",
    "\n",
    "    # remove str prefix of \"Project Number\" and make each entry an int instead of object\n",
    "    dframe['Project number'] = dframe['Project number'].str.replace('PME2-', '')\n",
    "    dframe['Project number'] = dframe['Project number'].astype(str).astype(np.int64)\n",
    "\n",
    "    # Remove NaNs / duplicates / outliers\n",
    "    dframe.dropna(inplace=True).reset_index(drop=True)\n",
    "    dframe.drop_duplicates(inplace=True)\n",
    "    dframe = dframe[(np.abs(zscore(dframe)) <= 3).all(axis=1)]\n",
    "\n",
    "    # Categorical variables: one-hot encoding\n",
    "    onehot_fields = ['Region']\n",
    "    for field in onehot_fields:\n",
    "        dummies = pd.get_dummies(dframe[field], prefix=field, drop_first=False)\n",
    "        dframe = pd.concat([dframe, dummies], axis=1)\n",
    "    dframe = dframe.drop(onehot_fields, axis = 1)\n",
    "\n",
    "    # Continuous variables: scaling\n",
    "    scaler = StandardScaler()\n",
    "    dframe = scaler.fit_transform(dframe.to_numpy())\n",
    "    \n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(actual,pred,scaler):\n",
    "    pred_inverse = scaler.inverse_transform(pred)\n",
    "    actual_inverse = scaler.inverse_transform(actual)\n",
    "\n",
    "    # Evaluate the model prediction\n",
    "    metrics = {\n",
    "        \"MAE\": mean_absolute_error(actual_inverse,pred_inverse),\n",
    "        \"MSE\": mean_squared_error(actual_inverse,pred_inverse),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(actual_inverse,pred_inverse))\n",
    "    }\n",
    "    \n",
    "    print(\"  Metrics: \")\n",
    "    for key, value in metrics.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_valid_split(dframe):\n",
    "    \"\"\"\n",
    "    we choose to split data with validation/test data to be at the end of time series\n",
    "    Parameters:\n",
    "        pandas.dataframe containing dataframe to split\n",
    "    Returns:\n",
    "        pandas.dataframe containing train/test/valiation data\n",
    "        pandas.dataframe containing valiation data\n",
    "        pandas.dataframe containing test data\n",
    "    \"\"\"\n",
    "    y = dframe.pop('Electricity produced by solar panels')\n",
    "\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(dframe, y, test_size=0.2, random_state=1, \n",
    "                                                        shuffle=True, stratify=dframe['Region'])\n",
    "\n",
    "    train_X, validation_X, train_Y, validation_Y = train_test_split(train_X, train_Y, test_size=0.25, random_state=1, \n",
    "                                                      shuffle=True, stratify=dframe['Region']) # 0.25 x 0.8 = 0.2\n",
    "    \n",
    "    return train_X, validation_X, test_X, train_Y, validation_Y, test_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the model and hyperparameters\n",
    "class Regression(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Regression  Techniques are used when the output is real-valued based on continuous variables. \n",
    "                For example, any time series data. This technique involves fitting a line\n",
    "    Feature: Features are individual independent variables that act as the input in your system. \n",
    "             Prediction models use features to make predictions. \n",
    "             New features can also be obtained from old features using a method known as ‘feature engineering’. \n",
    "             More simply, you can consider one column of your data set to be one feature. \n",
    "             Sometimes these are also called attributes. T\n",
    "             The number of features are called dimensions\n",
    "    Target: The target is whatever the output of the input variables. \n",
    "            In our case, it is the output value range of load. \n",
    "            If the training set is considered then the target is the training output values that will be considered.\n",
    "    Labels: Label: Labels are the final output. You can also consider the output classes to be the labels. \n",
    "            When data scientists speak of labeled data, they mean groups of samples that have been tagged to one or more labels.\n",
    "\n",
    "    ### The Model ### \n",
    "    Initialize the layers\n",
    "    Here we have:\n",
    "        one input layer (size 'lookback_window'), \n",
    "        one output layer (size 36 as we are predicting next 36 hours)\n",
    "        hidden layers define by 'params' argument of init\n",
    "    \"\"\"\n",
    "    def __init__(self, **params):\n",
    "        super(Regression, self).__init__()\n",
    "\n",
    "        # used by trainer logger (check log_graph flag)\n",
    "        # example of input use by model (random tensor of same size)\n",
    "        self.example_input_array = torch.rand(params['l_window'])\n",
    "\n",
    "        # self.loss = MeanAbsolutePercentageError() #MAPE\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "        # enable Lightning to store all the provided arguments \n",
    "        # under the self.hparams attribute. \n",
    "        # These hyperparameters will also be stored within the model checkpoint\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        #input dim set to lookback_window while output dim set to f_horizon\n",
    "        # input_dim, output_dim = self.hparams.l_window, self.hparams.f_horizon\n",
    "\n",
    "        \"\"\"\n",
    "        feature_extractor: all layers before classifier\n",
    "        classifier: last layer connecting output with rest of network (not always directly)\n",
    "        We load proper pretrained model, and use its feauture_extractor for the new untrained one\n",
    "        (Also check forward pass commentary)\n",
    "        \"\"\"\n",
    "        self.feature_extractor = None        \n",
    "        self.classifier = None\n",
    "\n",
    "        feature_layers, last_dim = self.make_hidden_layers()\n",
    "        self.feature_extractor = nn.Sequential(*feature_layers) #list of nn layers\n",
    "        self.classifier = nn.Linear(last_dim, 1)\n",
    "\n",
    "    def make_hidden_layers(self):\n",
    "        \"\"\"\n",
    "        Each loop is the setup of a new layer\n",
    "        At each iteration:\n",
    "            1. add previous layer to the next (with parameters gotten from layer_sizes)\n",
    "                    at first iteration previous layer is input layer\n",
    "            2. add activation function\n",
    "            3. set current_layer as next layer\n",
    "        connect last layer with cur_layer\n",
    "        Parameters: None\n",
    "        Returns: \n",
    "            layers: list containing input layer through last hidden one\n",
    "            cur_layer: size (dimension) of the last hidden layer      \n",
    "        \"\"\"\n",
    "        layers = [] # list of layer to add at NN\n",
    "        cur_layer = 1 #lookback window is of size 1\n",
    "\n",
    "        for next_layer in self.hparams.layer_sizes: \n",
    "            print(f'({cur_layer},{next_layer},{self.hparams.layer_sizes})')\n",
    "            layers.append(nn.Linear(cur_layer, next_layer))\n",
    "            layers.append(getattr(nn, self.hparams.activation)()) # nn.activation_function (as suggested by Optuna)\n",
    "            cur_layer = next_layer #connect cur_layer with previous layer (at first iter, input layer)\n",
    "        return layers, cur_layer\n",
    "\n",
    "    # Perform the forward pass\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In forward pass, we pass input through (freezed or not) feauture extractor\n",
    "        and then its output through the classifier \n",
    "        \"\"\"\n",
    "        representations = self.feature_extractor(x)\n",
    "        return self.classifier(representations)\n",
    "\n",
    "### The Data Loaders ###     \n",
    "    # Define functions for data loading: train / validate / test\n",
    "\n",
    "# If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, \n",
    "# you can speed up the host to device transfer by enabling \"pin_memory\".\n",
    "# This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer.    \n",
    "    def train_dataloader(self,train_X, train_Y):\n",
    "        feature = torch.tensor(train_X.values).float() #feature tensor train_X\n",
    "        target = torch.tensor(train_Y.values).float() #target tensor train_Y\n",
    "        train_dataset = TensorDataset(feature, target)  # dataset bassed on feature/target\n",
    "        train_loader = DataLoader(dataset = train_dataset, \n",
    "                                  shuffle = True, \n",
    "                                  pin_memory=True if torch.cuda.is_available() else False, #for GPU\n",
    "                                  num_workers = self.hparams.num_workers,\n",
    "                                  batch_size = self.hparams.batch_size)\n",
    "        return train_loader\n",
    "            \n",
    "    def test_dataloader(self,test_X,test_Y):\n",
    "        feature = torch.tensor(test_X.values).float()\n",
    "        target = torch.tensor(test_Y.values).float()\n",
    "        test_dataset = TensorDataset(feature, target)\n",
    "        test_loader = DataLoader(dataset = test_dataset, \n",
    "                                 pin_memory=True if torch.cuda.is_available() else False, #for GPU\n",
    "                                 num_workers = self.hparams.num_workers,\n",
    "                                 batch_size = self.hparams.batch_size)\n",
    "        return test_loader\n",
    "\n",
    "    def val_dataloader(self,validation_X,validation_Y):\n",
    "        feature = torch.tensor(validation_X.values).float()\n",
    "        target = torch.tensor(validation_Y.values).float()\n",
    "        val_dataset = TensorDataset(feature, target)\n",
    "        validation_loader = DataLoader(dataset = val_dataset,\n",
    "                                       pin_memory=True if torch.cuda.is_available() else False, #for GPU\n",
    "                                       num_workers = self.hparams.num_workers,\n",
    "                                       batch_size = self.hparams.batch_size)\n",
    "        return validation_loader\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "    \n",
    "### The Optimizer ### \n",
    "    # Define optimizer function: here we are using ADAM\n",
    "    def configure_optimizers(self):\n",
    "        return getattr(optim, self.hparams.optimizer_name)( self.parameters(),\n",
    "                                                            # momentum=0.9, \n",
    "                                                            # weight_decay=1e-4,                   \n",
    "                                                            lr=self.hparams.l_rate)\n",
    "\n",
    "### Training ### \n",
    "    # Define training step\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        # Add logging\n",
    "        logs = {'loss': loss}\n",
    "        self.log(\"loss\", loss, on_epoch=True) # computes train_loss mean at end of epoch        \n",
    "        return {'loss': loss, 'log': logs}\n",
    "\n",
    "### Validation ###  \n",
    "    # Define validation step\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"avg_val_loss\", loss, on_epoch=True)  # computes avg_loss mean at end of epoch\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "### Testing ###     \n",
    "    # Define test step\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        correct = torch.sum(logits == y.data)\n",
    "        # I want to visualize my predictions vs my actuals so here I'm going to \n",
    "        # add these lines to extract the data for plotting later on\n",
    "        self.log('test_loss', loss, on_epoch=True)        \n",
    "        return {'test_loss': loss, 'test_correct': correct, 'logits': logits}\n",
    "\n",
    "### Prediction ###\n",
    "    # Define prediction step\n",
    "        # This method takes as input a single batch of data and makes predictions on it. \n",
    "        # It then returns predictions\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        return self.forward(x)\n",
    "        \n",
    "    def on_train_epoch_end(self):\n",
    "        gc.collect()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
